% !TEX root = scombinatorics.tex
\documentclass[scombinatorics.tex]{subfiles}
\begin{document}
\chapter{Law(s) of large numbers}
\label{ulln}



\def\medrel#1{\parbox[t]{5ex}{$\displaystyle\hfil #1$}}
\def\ceq#1#2#3{\parbox[t]{20ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

Quoting some notes by Carlos C.~Rodr\'iguez

\begin{quotation}\parindent0mm\parskip1ex\noindent
What is a Law of Large Numbers?
I am glad you asked! The Laws of Large Numbers, or LLNs for short, come in
three basic flavors: Weak, Strong and Uniform. They all state that the observed
frequencies of events tend to approach the actual probabilities as the number
of observations increases. Saying it in another way, the LLNs show that under
certain conditions, we can asymptotically learn the probabilities of events from
their observed frequencies. To add some drama we could say that if God is
not cheating and S/he doesn’t change the initial standard probabilistic model
too much then, in principle, we (or other machines, or even the universe as a
whole) could eventually find out the Truth, the whole Truth, and nothing but
the Truth.

Bull! The Devil, is in the details.

I suspect that for reasons not too different in spirit to the ones above, famous
minds of the past took the slippery slope of defining probabilities as the limits
of relative frequencies. They became known as “frequentists”. They wrote
books and indoctrinated generations of confused students.
\end{quotation}

%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{Inequalities}

Throughout this and the next section we work with a given sample space $\Omega,\Pr$.
For simplicity, the following two propositions are proved for finite $\Omega$, but they are easily seen to hold in  general.
The finiteness will be an essential hypothesis In Section~\ref{uniform}.

A function $f:\RR\to\RR$ is \emph{convex\/} if for every tuples of real numbers $p_i$ and $x_i$ such that

\ceq{\hfill\sum^n_{i=1}p_i}
{=}
{1}

we have

\ceq{\hfill f\bigg(\sum^n_{i=1}p_ix_i\bigg)}
{\le}
{\sum^n_{i=1}p_if(x_i)}\smallskip

Note that, though the definition is usually given with $n=2$, the general property above follows easily.

\begin{void_thm}[Jensen's inequality]\label{Jensen}
Let $f:\RR\to\RR$ be a convex function.
  Then

  \ceq{\hfill f\Big(\Ex[X]\Big)}
  {\le}
  {\Ex\Big[f(X)\Big]}
\end{void_thm}


\begin{proof}
  For simplicity, assume that the sample space $\Omega$ is finite.
  Then the claim is obvious from the definition.
\end{proof}

The following is arguably the most basic inequality in probability theory.
Although it is almost trivial, it will be required several times in this chapter. 

\begin{void_thm}[Markov's inequality]\label{Markov}
  Let $X$ be a nonnegative random variable with finite mean.
  Then for every $\epsilon>0$

  \ceq{\hfill\Pr\Big(X\ge\epsilon\Big)}
  {\le}
  {\frac{\Ex[X]}{\epsilon}}
\end{void_thm}

\begin{proof}
  For simplicity, assume that the sample space $\Omega$ is finite.
  (The theorem holds in general, but we only need the finite case.)
  Define $A=\{a\in\Omega\,:\, X(a)\ge\epsilon\}$.

  \ceq{\hfill\Ex[X]}{=}{\sum_{a\in\Omega} \Pr(a)\, X(a)}

  \ceq{}{=}{\sum_{a\in A} \Pr(a)\, X(a) + \sum_{a\notin A} \Pr(a)\, X(a)}

  \ceq{}{\ge}{\sum_{a\in A} \Pr(a)\, X(a)}

  \ceq{}{\ge}{\epsilon\sum_{a\in A}\Pr(a)}

  \ceq{}{=}{\epsilon\,\Pr(X\ge\epsilon)}
\end{proof}

\begin{corollary}
  Let $X$ be a nonnegative random variable.
  If $\Ex\big[X^k\big]$ exists, then for every $\epsilon>0$
  
  \ceq{\hfill\Pr\Big(X\ge\epsilon\Big)}
  {\le}
  {\frac{\Ex[X^k]}{\epsilon^k}}
\end{corollary}

\begin{proof}
  By Markov's inequality, since $\Pr\big(X\ge\epsilon\big)=\Pr\big(X^k\ge\epsilon^k\big)$.
\end{proof}

Chebyshev's inequality (a.k.a. Chebysheff, Chebyshov, Tschebyscheff, Tschebycheff) is a special case of the corollary above.

\begin{void_thm}[Chebyshev's inequality]\label{Chebyshev}
    Let $X$ be a random variable with finite mean and variance.
    Then for every $\epsilon>0$
    
    \ceq{\hfill\Pr\Big(\big|X-\Ex[X]\big|\ge\epsilon\Big)}
    {\le}
    {\frac{\Var[X]}{\epsilon^2}}\QED
\end{void_thm}

To obtain exponential bounds, we frequently apply the following trick.

\begin{void_thm}[Chernoff's method]\label{lem_chernoff_method}
  Let $X$ be a random variable with finite mean.
  Then for every $t>0$

  \ceq{\hfill\Pr\big(X\ge\epsilon\big)}
  {\le}
  {e^{-t\epsilon}\,\Ex\big[e^{tX}\big]}
\end{void_thm}

\begin{proof}
  For  every $t>0$

  \ceq{\hfill\Pr\big(X\ge\epsilon\big)}
  {=}
  {\Pr\big(e^{tX}\ge e^{t\epsilon}\big)}
  \hfill because $e^{tx}$ is increasing
  
  \ceq{}
  {\le}
  {e^{-t\epsilon}\,\Ex\big[e^{tX}\big],}

  by Markov's inequality, which me may apply since $e^{tX}$ is always positive. 
\end{proof}


  \begin{void_thm}[Hoeffding's lemma]\label{lem_Hoeffding}
    Let $X$ be a bounded random variable, say $a\le X\le b$. 
    Let $\Ex[X]=\mu$ and $d=b-a$.
    Then
    
    \ceq{\hfill\Ex\Big[e^{t(X-\mu)}\Big]}
    {\le}
    {\exp{\Big(\frac{t^2d^2}8}\Big).}\smallskip
  \end{void_thm}

\begin{proof}
  For clarity, assume $\mu=0$.
  The general result follows easily from this special case by centralization.
  Recall that, by convexity, for every $x\in[a,b]$

  \ceq{\hfill e^{tx}}
  {\le}
  {\frac{x-a}{d}\ e^{tb}\ +\ \frac{b-x}{d}\ e^{ta}}

  Then

  \ceq{\hfill e^{tX}}
  {\le}
  {\frac{X-a}{d}\ e^{tb}\ +\ \frac{b-X}{d}\ e^{ta}}

  By the linearity of expectation,

  \ceq{\hfill\Ex\Big[e^{tX}\Big]}
  {\le}
  {\frac{b\,e^{ta}-a\,e^{tb}}{d}}

  \ceq{\hfill\log\Ex\Big[e^{tX}\Big]}
  {\le}
  {\log\frac{b\,e^{ta}-a\,e^{tb}}{d}}

taking the Taylor series expansion of the r.h.s.\@ at $t=0$ we obtain (the first and second derivatives vanish at $0$; the second derivative is always $\le 1/4$)

\ceq{\hfill\log\Ex\Big[e^{tX}\Big]}
    {\le}
    {\frac{t^2d^2}8.}
\end{proof}

\begin{void_thm}[Hoeffding's inequality]\label{Hoeffding_inequality}
  Let $X_1,\dots,X_n$ be independent random variables with bounded range, say $a\le X_i\le b$. Define $d=b-a$.
  
  \ceq{\hfill M}{=}{\sum^n_{i=1}\Big(X_i-\Ex[X_i]\Big)}
  
  Then for every $\epsilon>0$ 

  \ceq{\hfill\Pr \Big(M\ge\epsilon\Big)}
      {\le}
      {\exp{\Big(-\frac{2\epsilon^2}{nd^2}}\Big),}

  \ceq{\hfill\Pr \Big(M\le-\epsilon\Big)}
      {\le}
      {\exp{\Big(-\frac{2\epsilon^2}{nd^2}}\Big).}\smallskip
\end{void_thm}

Clearly, the two inequalities above imply the following

\ceq{\hfill\Pr \Big(|M|\ge\epsilon\Big)}
    {\le}
    {2\exp{\Big(-\frac{2\epsilon^2}{nd^2}}\Big).}\smallskip


\begin{proof}
  Define $\Ex[X_i]=\mu_i$.
  Let $t>0$ be arbitrary.

  \ceq{\hfill \Pr\Big(M\ge\epsilon\Big)}{\le}{e^{-t\epsilon}\,\Ex\Big[e^{tM}\Big]}\hfill by Chernoff's method (\ref{lem_chernoff_method})

  \ceq{}
      {=}
      {e^{-t\epsilon}\,\prod^n_{i=1}\Ex\Big[e^{t\,(X_i-\mu_i)}\Big]}
      \hfill by independence.


      \ceq{}
      {\le}
      {e^{-t\epsilon}\,\prod^n_{i=1}\exp\Big(\frac{t^2d^2}{8}\Big)}
      \hfill by Hoeffding's Lemma (\ref{lem_Hoeffding}).

  \ceq{}
      {=}
      {\exp\Big(\frac{n\,t^2d^2}{8}-t\epsilon\Big)}

  Now substitute $4\epsilon/nd^2$ for $t$.
\end{proof}



We prove Hoeffding's lemma with a slightly weaker bound ($2$ for $8$).
The purpose is to present a clever trick called \textit{symmetrization\/} which in the following section is applied in a more complex setting.

First we need the following lemma.


\begin{lemma}\label{lem_sign}
  Let $\sigma$ be a random sign variable (a.k.a.\@ Rademacher random variable).
  That is, $\sigma\in\{-1,1\}$ with uniform distribution.
  Then for every $t>0$

  \ceq{\hfill\Ex\Big[e^{t\sigma}\Big]}
      {\le}
      {e^{t^2/2}}
\end{lemma}

\begin{proof}
  Replace $e^x$ with its Taylor expansion around $x=0$

  \ceq{\hfill\Ex\Big[e^{t\sigma}\Big]}
  {=}
  %     {\Ex\Bigg[\sum^\infty_{i=0}\frac{(t\sigma)^i}{i!}\Bigg]}
  %
  % \ceq{}
  %     {=}
  {\sum^\infty_{i=0}\frac{t^i\Ex\big[\sigma^i\big]}{i!}}

  \ceq{}
  {=}
  {\sum^\infty_{i=0}\frac{t^{2i}}{(2i)!}}
  \hfill since
  $\Ex\big[\sigma^i\big]=\bigg\{\kern-1ex
  \begin{array}{ll}1&i{\rm\ even}\\0&i{\rm\ odd}\end{array}$

  \ceq{}
  {=}
  {\sum^\infty_{i=0}\frac{(t/2)^{2i}}{i!}}

  \ceq{}
  {=}
  {e^{t^2/2}.}
\end{proof}

\begin{void_def}[Second proof of Hoeffding's Lemma]\label{proof_2nd_Hoeffding}
  Recall that Hoeffding's Lemma claims that, if $a\le X\le b$, then
  
  \ceq{\hfill\Ex\Big[e^{t(X-\mu)}\Big]}
  {\le}
  {\exp{\Big(\frac{t^2d^2}8}\Big).}

  where $\Ex[X]=\mu$ and $d=b-a$.

  Let $X'$ be an independent copy of $X$.
  In particular $\mu=\Ex(X')$.
  Then

  \ceq{\hfill\Ex\Big[e^{t(X-\mu)}\Big]}
  {=}
  {\Ex\Big[e^{t(X-\Ex[X'])}\Big]}


  \ceq{}
  {\le}
  {\Ex\Big[\Ex\big[e^{t(X-X')}\, |\, X\big]\Big]}\hfill by Jensen's inequality

  \ceq{}
  {\le}
  {\Ex\Big[e^{t(X-X')}\Big]}

  Let $\sigma$ be a random sign variable independent of $X,X'$.
  % That is, $\sigma\in\{-1,1\}$ with uniform distribution.
  % Assume also that $\sigma$ is independent of $X,X'$.
  Then $\sigma(X-X')$ has the same distribution of $X-X'$.

  \ceq{}
  {=}
  {\Ex\Big[e^{t\sigma(X-X')}\Big]}

  \ceq{}
  {=}
  {\Ex\bigg[\Ex\Big[e^{t\sigma(X-X')}\ |\ X,X'\Big]\bigg]}
  
  \ceq{}
  {\le}
  {\Ex\Big[e^{t^2(X-X')^2/2}\Big]}
  \hfill by Lemma~\ref{lem_sign}
      
  \ceq{}
  {\le}
  {e^{t^2d^2/2}}
  \hfill  because $|X-X'|\le d$.
  
This yields the bound above (only with 2 in place of 8).\QED
\end{void_def}

%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{Two Weak Laws of Large Numbers}\label{samples}

\def\smallcirc{\mathord{\kern-.2ex\raisebox{.4ex}{$\scriptscriptstyle\circ$}}}
\def\Indicator{{\mathds I}}


A \emph{sample\/} $s$ is a sequence $s_0,\dots,s_{n-1}$ of elements of $\Omega$.
Its length $|s|=n$ is also called \emph{size\/} or \emph{dimension.}
We write $\range(s)$ for the set $\{s_0,\dots,s_{n-1}\}$.
Note that this set may have cardinality $<n$.

To a sample $s$ of size $n$ we associate a finite probability measure on the subsets of $\Omega$, namely for any $A\subseteq\Omega$ we define 

\ceq{\hfill\emph{$\displaystyle\Fr\big(s, A\big)$}}
{=}
{\frac1n\cdot \big|\big\{i<n\ :\ s_i\in A\big\}\big|.}

Let $S=S_1,\dots,S_n$ be independent random elements of $\Omega$, that is, random variables such that $\Pr(S_i\in A)=\Pr(A)$ for every $A\subseteq\Omega$.
Write $\Indicator_A$ for the indicator function of $A$. 
Then $\Indicator_A\smallcirc S_i$ as a Bernoulli random variable with probability of success $\Pr(A)$.
Moreover

\ceq{\hfill \Fr(S,A)}{=}{\frac1n\sum^n_{i=1}\Indicator_A\smallcirc S_i.}

\begin{void_thm}[Weak Law of Large Numbers]
  For every event $A\subseteq\Omega$ and every $n>0$
  
  \ceq{\hfill \frac1{n\epsilon^2}}
  {\ge}
  {\Pr \Big(s\in\Omega^n\ :\ \big|\Fr(s,A) - \Pr(A)  \big|\ge\epsilon\Big).}
\end{void_thm}


\begin{proof}
  Let $S_1,\dots,S_n$ be independent random elements of $\Omega$.
  Up to the factor $1/n$, the distribution of $\Fr(S,A)$ is binomial with parameters $n$ and $\Pr(A)$.
  Therefore it has expected value $\Pr(A)$ and  variance $\le1/n$. 
  By Chebyshev's inequality we obtain 
  
  \ceq{\hfill \frac1{n\epsilon^2}}
  {\ge}
  {\Pr\Big(\big|\Fr(S,A) - \Pr(A)\big|\ge\epsilon\Big)}

  which proves the theorem.
\end{proof}

Sometime we are interested in the minimal size of a sample that approximates the probability up to a given $\epsilon$.

\begin{corollary}\label{corl_wlln}
  For every $A\subseteq\Omega$ and every $\epsilon>0$ there is a sample $s$ of size

  \ceq{\hfill |s|}
      {=}
      {\left\lfloor\frac1{\epsilon^2}+1\right\rfloor} 
      
  such that

  \ceq{\hfill\epsilon}{>}{\Big|\Fr(s,A) - \Pr(A) \Big|.}
\end{corollary}


\begin{proof}
  By the Weak Law of Large Numbers above, a sample of size $n$ exists if

  \ceq{\hfill 1}
      {>}
      {\frac1{n\epsilon^2}}
\end{proof}

In the following section we need a better bound for the Weak Law of Large Numbers.
This is obtained with a similar proof.

\begin{void_thm}[Weak Law of Large Numbers (with exponential bound)]
  For every event $A\subseteq\Omega$ and every $n>0$
  
  \ceq{\hfill 2 e^{-2n\epsilon^2}}
  {\ge}
  {\Pr \Big(s\in\Omega^n\ :\ \big|\Fr(s,A) - \Pr(A)  \big|\ge\epsilon\Big).}
\end{void_thm}

\begin{proof}
  Let $S_1,\dots,S_n$ be independent random elements of $\Omega$.
  Define
  
  \ceq{\hfill M}
  {=}
  {\sum^n_{i=1}\Big(\Indicator_A\smallcirc S_i-\Ex[\Indicator_A\smallcirc S_i]\Big)}

  As $\Ex[\Indicator_A\smallcirc S_i]=\Pr(A)$, the inequality we have to prove can be rewritten as 

  \ceq{\hfill 2\, e^{-2n\epsilon^2}}
  {\ge}
  {\Pr \Big(|M|\ge n\epsilon\Big)}

  and this follows immediately from Hoeffding inequality.
  \end{proof}


Using the exponential bounds above, we can improve (by a constant factor) the size of the minimal sample size that approximates the probability obtained in Corollary~\ref{corl_wlln}.

\begin{corollary}
  For every $A\subseteq\Omega$ and every $\epsilon>0$ there is a sample $s$ of size

  \ceq{\hfill|s|}
      {=}
      {\left\lfloor\frac{\log 2}{2\epsilon^2}+1\right\rfloor} 
      
  such that

  \ceq{\hfill\epsilon}{>}{\Big|\Fr(s,A) - \Pr(A) \Big|.}\QED
\end{corollary}


%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%
\section{The Uniform Law of Large Numbers}\label{uniform}
\def\medrel#1{\parbox[t]{5ex}{$\displaystyle\hfil #1$}}
\def\ceq#1#2#3{\parbox[t]{35ex}{$\displaystyle #1$}\medrel{#2}{$\displaystyle #3$}}

In this section we work with a fixed formula $\phi(x\,;z)$ and with the family of definable subsets $\phi(\Omega\,;b)_{b\in\V}$ of a given finite sample space $\Omega,\Pr$.
It is convenient to introduce e some abbreviations

\ceq{\hfill \Pr(b)}
{=}
{\Pr\Big(\phi(\Omega\,;b)\Big)}

\ceq{\hfill\Fr(s,b)}
{=}
{\Fr\Big(s,\phi(\Omega\,;b)\Big)}

An \emph{$\epsilon$-approximation\/} is a sample $s$ such that

\ceq{\hfill\Big|\Fr(s,b) -\Pr(b)\Big|}
{<}
{\epsilon}
\hfill for every $b\in\V$.

We are interested in estimating the minimal size of an $\epsilon$-approximation.

The main theorem of this section is this famous result of Vapnik-Chervonenkis~\cite{VC}.
Following Devroye and Lugosi~\cite{DL}, we prove a slightly better bound.
 

\begin{void_thm}[Vapnik-Chervonenkis inequality]\label{VC_inequality}
  Let $\pi_\phi(n)$ be the shatter function of $\phi(\Omega\,;b)_{b\in\V}$.
  Let $S=S_1,\dots,S_n$ be independent random elements of $\Omega$.
  Then, for every $b\in\V$

  \ceq{\hfill\Ex\,\Big|\Fr(S,b) - \Pr(b)\Big|}
  {<}
  {2\sqrt{\frac{\log\;2\pi_\phi(2n)}{n}}}.\QED
\end{void_thm}

Before embarking of the proof of the theorem above, we prove the following easy but mysterious lemma which is of independent interest.

\begin{lemma}\label{lem_mistero}
  Let $X_1,\dots,X_m$ real valued random variable (the sample space $\Omega,\Pr$ is arbitrary).
  Let $c$ be such that 

  \ceq{\hfill\Ex\big[e^{tX_i}\big]}
  {\le}
  {e^{t^2c^2/2}}
  \hfill for every $i\le m$ and every $t>0$.

  Then 

  \ceq{\hfill\Ex\big[\max_{i\le m} X_i\big]}
  {\le}
  {c\sqrt{2\log m}}.
\end{lemma}

\begin{proof}
  By Jensen's inequality,

  \ceq{\hfill\exp\Big(t\cdot \Ex\big[\max_{i\le m} X_i\,\big]\Big)}
  {\le}
  {\Ex\Big[\exp\!\big(\max_{i\le m} t X_i\,\big)\Big]}

  \ceq{}
  {=}
  {\Ex\Big[\max_{i\le m} e^{t X_i}\Big]}

  \ceq{}
  {=}
  {\Ex\Big[\sum_{i\le m} e^{t X_i}\Big]}

  \ceq{}
  {=}
  {\sum_{i\le m}\Ex\big[ e^{t X_i}\big]}

  \ceq{}
  {=}
  {m\,nullae^{t^2c^2/2}}

The lemma is obtained taking the logarithm and substituting $t=\dfrac{\sqrt{2\log m}}{c}$.
\end{proof}

\textbf{Proof of the Vapnik-Chervonenkis inequality}
Let $S'=S'_1,\dots,S'_n$ be an independent copy of $S$.
We claim that

\ceq{(1)\hfill\Ex\Big[\sup_{b\in\V}\big|\Fr(S,b) - \Pr(b)\big|\Big]}
{\le}
{\Ex\Big[\sup_{b\in\V}\big|\Fr(S,b) - \Fr(S',b)\big|\Big]}

In fact,

\ceq{\hfill\Fr(S,b) - \Pr(b)}
{=}
{\Fr(S,b) - \Ex\big[\Fr(S',b)\big]}

\ceq{}
{=}
{\Ex\Big[\Fr(S,b) - \Fr(S',b)\ \big|\ S\Big].}

Now, apply Jensen's inequality to the absolute value function, then use that 

\ceq{\hfill\sup_{b\in\V}\Ex\big[\cdots\big]}
{\le}
{\Ex\big[\sup_{b\in\V}(\cdots)\big].}

Write $\Indicator_b$ for the indicator function of $\phi(\Omega)$.
Then

\ceq{\hfill\big|\Fr(S,b) - \Fr(S',b)\big|}
{=}
{\frac1n\bigg|\sum^n_{i=1} \Big(\Indicator_b\smallcirc S_i -  \Indicator_b\smallcirc S'_i\Big)\bigg|}

Let $\sigma=\sigma_1,\dots,\sigma_n$ be a tuple of independent sign random variable.
The random variable $\Indicator_b\smallcirc S_i -  \Indicator_b\smallcirc S'_i$ has the same distribution of $\sigma_i\big(\Indicator_b\smallcirc S_i -  \Indicator_b\smallcirc S'_i\big)$ hence

\ceq{}
{=}
{\frac1n\Ex\bigg|\sum^n_{i=1} \sigma_i\Big(\Indicator_b\smallcirc S_i -  \Indicator_b\smallcirc S'_i\Big)\ \Big|\ S,S'\bigg|}

Inserting this into (1) we obtain

% \ceq{\hfill\sup_{b\in\V}\Ex\bigg|\sum^n_{i=1} \sigma_i\big(\Indicator_b\,s_i -  \Indicator_b\,s'_i\big)\bigg|}
% {}
% {}

\ceq{\hfill\Ex\Big[\sup_{b\in\V}\big|\Fr(S,b) - \Pr(b)\big|\Big]}
{\le}
{\frac1n\,\Ex\bigg[\sup_{b\in\V}\Ex\bigg|\sum^n_{i=1} \sigma_i\Big(\Indicator_b\smallcirc S_i -  \Indicator_b\smallcirc S'_i\Big)\ \Big|\ S,S'\bigg|\bigg]}

Let $s,s'$ be a generic realization of $S,S'$


\ceq{}
{\le}
{\frac1n\,\sup_{s,s'}\,\sup_{b\in\V}\Ex\bigg|\sum^n_{i=1} \sigma_i\big(\Indicator_b\,s_i -  \Indicator_b\,s'_i\big)\bigg|}

\ceq{}
{\le}
{\frac1n\,\sup_{s,s'}\Ex\bigg[\sup_{b\in\V}\bigg|\sum^n_{i=1} \sigma_i\big(\Indicator_b\,s_i -  \Indicator_b\,s'_i\big)\bigg|\bigg]}

Observe that once $s,s'$ is fixed, $\sup_{b\in\V}$ is actually a maximum among $m=\pi_\phi(2n)$ sets, that is,the definable subsets of $\{s_1,\dots,s_n,s'_1,\dots,s'_n\}$.
Then, by Lemma~\ref{lem_mistero}, for the appropiate $c$ we obtain

\ceq{}
{\le}
{\frac1n\,\sup_{s,s'}c\sqrt{2\log 2\pi_\phi(2n)}.}

Finally, as the l.h.s.\@ does not depend on $s,s'$

\ceq{}
{\le}
{\frac{c}n\sqrt{2\log\big(2\,\pi_\phi(2n)\big).}}

We are only left with proving that the assumption of Lemma~\ref{lem_mistero} holds with $c=\sqrt{n}$.

\ceq{\hfill\Ex\bigg[\exp\bigg(t\sum^n_{i=1} \sigma_i\big(\Indicator_b\,s_i -  \Indicator_b\,s'_i\big)\bigg)\bigg]}
{=}
{\prod^n_{i=1} \Ex\bigg[\exp\Big(t\,\sigma_i\big(\Indicator_b\,s_i -  \Indicator_b\,s'_i\big)\Big)\bigg]}

As $\sigma_i\big(\Indicator_b\,s_i -  \Indicator_b\,s'_i\big)$ takes values in $\{-1,1\}$ with mean $0$, by Lemma~\ref{lem_sign}

\ceq{}
{\le}
{e^{nt^2/2}.}\QED
\end{document}
